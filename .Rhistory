mongo.get.database.collections(db, 'db')
mongo.get.database.collections(db, 'locations')
mongo.get.database.collections(db, 'indicators')
mongo.get.database.collections(db)
mongo.get.database.collections(db, 'gaza')
mongo.get.databases(db)
mongo.get.database.collections(db, 'gaza')
?rmonbodb
?rmongodb
??rmongodb
mongo.is.connected(db)
if (mongo.is.connected(mongo)) message('Db connection alright.')
else message('Check connection.')
if (mongo.is.connected(db)) message('Db connection alright.')
else message('Check connection.')
# small connection test
if (mongo.is.connected(db)) message('Db connection alright.')
else message('Check connection.')
if (!mongo.is.connected(db)) message('Db connection alright.')
if (!mongo.is.connected(db)) message('Issue: Check connection.')
if (!mongo.is.connected(db)) stop('Issue: Check database connection.')
mongo.get.database.collections(db, 'gaza')
mongo.get.database.collections(db, 'gaza')
mongo.count(db, 'gaza.values')
mongo.count(db, 'gaza.datasets')
mongo.count(db, 'gaza.locatiosn')
mongo.count(db, 'gaza.locations')
mongo.count(db, 'gaza.indicators')
indicators <- mongo.find.all(db, 'gaza.indicators')
View(indicators)
indicators <- mongo.find.all(db, 'gaza.indicators')
getIndicators <- function() {
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <- mongo.find.all(db, 'gaza.indicators')
}
}
getIndicators <- function() {
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <<- mongo.find.all(db, 'gaza.indicators')
}
}
getIndicators()
View(indicators)
getIndicators <- function() {
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <<- mongo.find.all(db, 'gaza.indicators')
message('Use View(indicators) to see the results.')
}
}
getIndicators()
View(indicators)
View(indicators)
indicators <- mongo.find.all(db, 'gaza.ingest')
View(indicators)
# get the data back from a particular table
getCollection <- function(col = NULL) {
# test
if (is.null(col)) stop('provide a collection name')
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <<- mongo.find.all(db, paste0('gaza.', col))
message('*****Use View(indicators) to see the results.*****')
}
}
# get the data back from a particular table
getCollection <- function(col = NULL) {
# test
if (is.null(col)) stop('provide a collection name')
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <<- mongo.find.all(db, paste0('gaza.', col))
message('*****Use View() to see the results.*****')
}
}
getCollection()
getCollection('indicators')
# get the data back from a particular table
getCollection <- function(col = NULL) {
# test
if (is.null(col)) stop('provide a collection name')
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
mongo.find.all(db, paste0('gaza.', col))
message('*****Use View() to see the results.*****')
}
}
getCollection()
getCollection('indicator')
getCollection('indicators')
# get the data back from a particular table
getCollection <- function(col = NULL) {
# test
if (is.null(col)) stop('provide a collection name')
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
x <- mongo.find.all(db, paste0('gaza.', col))
message('*****Use View() to see the results.*****')
}
return(x)
}
getCollection('ingest')
library(XML)
library(RCurl)
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en', sep="")
year = 2014
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en', sep="")
list_url
# getting a list of documents
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
list_url
table <- getNodeSet(htmlParse(list_url),"//li")[[1]]
table
table <- getNodeSet(htmlParse(list_url),'//*[@id="content"]/div/div[1]/ul/li')[[1]]
table
doc <- readHTMLTable(table, useInternal = TRUE)
doc
class(doc)
doc <- htmlParse(list_url)
doc
link_it <- data.frame(a =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li', xmlGetAttr, 'href'))
link_it
doc <- htmlParse(list_url)
link_it <- data.frame(date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li', xmlGetAttr))
link_it <- data.frame(date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li', xmlGetValue))
?xpathSApply
link_it <- data.frame(date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li', xmlValue))
link_it
View(link_it)
link_it <- data.frame(date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue))
View(link_it)
link_it <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue)
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, 'href')
)
link_it <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, 'href')
)
View(link_it)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
message('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, 'href'),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year,
)
return(output)
}
updateList <- scrapeList()
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a[@href]', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year,
)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a[@href]', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year
)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year
)
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"))
View(output)
x <- data.frame(name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue))
View(x)
nrow(x)
nrow(output)
View(x)
View(output)
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
View(output)
nrow(output)
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue)
year = year)
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year)
View(x)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
message('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href")
year = year)
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
return(output)
}
updateList <- scrapeList()
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
message('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
return(output)
}
updateList <- scrapeList()
View(updateList)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
message('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat(' done')
return(output)
}
updateList <- scrapeList()
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
print('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat(' done')
return(output)
}
updateList <- scrapeList()
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
cat('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat(' done')
return(output)
}
updateList <- scrapeList()
View(updateList)
View(updateList)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
cat('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat('done.')
# fixing URLs
output$url <- sub('/entity/', 'http://www.who.int/', output$url)
return(output)
}
updateList <- scrapeList()
VieW(updateList)
View(updateList)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
cat('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat('done.')
# fixing URLs
output$url <- sub('/entity/', 'http://www.who.int/', output$url)
output$url <- sub('index.html', '', output$url)
return(output)
}
updateList <- scrapeList()
View(updateList)
View(updateList)
library(RCurl)
library(XML)
letters
class(letters)
for (i in 1:letters) {
print(toupper(letter[i]))
}
let <- as.list(letters)
let
for (i in 1:letters) {
print(toupper(let[i]))
}
for (i in 1:length(letters)) {
print(toupper(let[i]))
}
for (i in 1:length(letters)) {
print(toupper(letters[i]))
}
for (i in 1:length(letters)) {
print(toupper(letters[i]))
}
for (i in 1:length(letters)) {
query_url <- paste0(base_url, toupper(letters[i]))
}
base_url = "http://www.imsdb.com/alphabetical/"
for (i in 1:length(letters)) {
query_url <- paste0(base_url, toupper(letters[i]))
}
for (i in 1:length(letters)) {
query_url <- paste0(base_url, toupper(letters[i]))
print(paste0(base_url, toupper(letters[i])))
}
setwd("~/Documents/Programming/idps_mali_map")
library(reshape2)
library(ggplot2)
data <- read.csv('data/output_2.csv')
x <- melt(data)
# cleaning
x$variable <- sub("X", "", x$variable)
x$variable <- sub("\\.", "-", x$variable)
x$variable <- sub("\\.", "-", x$variable)
x$variable <- as.Date(x$variable)
names(x) <- c('name', 'date', 'value')
# casting
y <- dcast(x, date ~ name)
View(y)
ncol(y)
names(y)
a <- y[1, 2:ncol(y)] + y[1 + 1, 2:ncol(y)]
View(x)
View(a)
View(y)
# making cumulative calculation
for (i in 1:nrow(y)) {
z[i, ] <- y[i, 2:ncol(y)] + y[i + 1, 2:ncol(y)]
}
z <- data.frame()
for (i in 1:nrow(y)) {
z[i, ] <- y[i, 2:ncol(y)] + y[i + 1, 2:ncol(y)]
}
View(z)
warnings()
z <- y
for (i in 1:nrow(y)) {
z[i, ] <- y[i, 2:ncol(y)] + y[i + 1, 2:ncol(y)]
}
for (i in 1:nrow(y)) {
z[i, 2:ncol(z)] <- y[i, 2:ncol(y)] + y[i + 1, 2:ncol(y)]
}
View(z)
z <- y
for (i in 2:nrow(y)) {
z[i, 2:ncol(z)] <- y[i, 2:ncol(y)] + y[i + 1, 2:ncol(y)]
}
View(z)
z <- y
View(z)
z <- y
for (i in 1:nrow(y)) {
z[i + 1, 2:ncol(z)] <- y[i, 2:ncol(y)] + y[i + 1, 2:ncol(y)]
}
View(z)
VieW(y)
View(y)
y[i, 2:ncol(y)]
y[i, 2:ncol(y)] + y[i + 1, 2:ncol(y)]
i
z <- y
for (i in 1:nrow(y)) {
z[i + 1, 2:ncol(z)] <- z[i, 2:ncol(z)] + z[i + 1, 2:ncol(z)]
}
View(z)
nrow(z)
# making cumulative calculation
z <- y
for (i in 1:(nrow(z)-1)) {
z[i + 1, 2:ncol(z)] <- z[i, 2:ncol(z)] + z[i + 1, 2:ncol(z)]
}
View(z)
write.csv(z, 'http/idp_data_cummulative.csv', row.names = F)
View(y)
y$total <- y[, sum(2:ncol(y))]
y$total <- sum(y[, 2:ncol(y)])
View(Y)
View(y)
# calculating totals
for(i in 1:nrow(y)) {
y$total[i] <- sum(y[i, 2:ncol(y)])
}
y <- dcast(x, date ~ name)
names(y)
# calculating totals
for(i in 1:nrow(y)) {
y$total[i] <- sum(y[i, 2:ncol(y)])
}
View(y)
# casting
y <- dcast(x, date ~ name)
# calculating totals
for(i in 1:nrow(y)) {
y$Total[i] <- sum(y[i, 2:ncol(y)])
}
View(y)
write.csv(y, 'http/data/idp_data.csv', row.names = F)
View(y)
